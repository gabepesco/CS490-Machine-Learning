{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Collaborative Filtering and the Million Playlist Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justin Moczynski, Gabe Pesco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many famous artists whose work is frequently listened to on the radio and on streaming services, such as Spotify and Apple Music; however, these are not the only artists in the world. What about the other artists whose work may be more local or in the beginning stages of their careers? How does this affect an artist's appearance in music recommendations? In this project, we begin to examine the effect of a song's popularity on its appearance to listeners using a recommender system to dictate their future choices of music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this project was the Million Playlist Dataset (MPD) provided by Spotify for their Recommender Systems Challenge. The first obstacle was importing the data and converting it into a suitable form for use within the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "This code is only run once for the entire project because its purpose is to extract data and convert it into an npz file for use during the program. Once the npz file has been created, it does not need to be created multiple times since we hold a reference to the original file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the following libraries were imported to use in the data conversion process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import implicit\n",
    "import scipy.sparse\n",
    "import progressbar as pb\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "from sklearn import *\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Importing Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we began importing the data from the .json files. In order to do this, we used file scanners to read the contents of each file and dictionary, list, and set data structures to contain the scanned contents. For this project, we are using 2 of the given JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,00:00:03\t2,00:00:03\t4,00:00:03\t\n",
      "\n",
      "\n",
      "1,00:00:03\t3,00:00:03\t\n",
      "\n",
      "5,00:00:03\t6,00:00:03\t\n",
      "\n",
      "7,00:00:03\t8,00:00:03\t\n",
      "9,00:00:02\t\n",
      "\n",
      "10,00:00:03\t\n",
      "11,00:00:03\t12,00:00:03\t13,00:00:03\t\n",
      "\n",
      "\n",
      "14,00:00:03\t\n",
      "15,00:00:02\t\n",
      "18,00:00:02\t16,00:00:03\t\n",
      "\n",
      "19,00:00:02\t\n",
      "17,00:00:04\t\n",
      "20,00:00:02\t\n",
      "22,00:00:03\t21,00:00:03\t\n",
      "\n",
      "23,00:00:03\t24,00:00:02\t\n",
      "25,00:00:01\t\n",
      "\n",
      "26,00:00:02\t\n",
      "27,00:00:02\t28,00:00:00\t\n",
      "\n",
      "29,00:00:02\t\n",
      "30,00:00:03\t\n",
      "32,00:00:02\t31,00:00:03\t\n",
      "33,00:00:02\t\n",
      "\n",
      "35,00:00:02\t\n",
      "36,00:00:02\t\n",
      "34,00:00:03\t37,00:00:02\t\n",
      "\n",
      "38,00:00:03\t\n",
      "39,00:00:01\t\n",
      "40,00:00:02\t\n",
      "44,00:00:02\t41,00:00:02\t\n",
      "\n",
      "43,00:00:02\t42,00:00:03\t\n",
      "\n",
      "45,00:00:00\t\n",
      "46,00:00:02\t\n",
      "48,00:00:03\t47,00:00:03\t\n",
      "\n",
      "49,00:00:03\t50,00:00:03\t\n",
      "\n",
      "51,00:00:02\t52,00:00:03\t\n",
      "\n",
      "53,00:00:03\t54,00:00:02\t\n",
      "55,00:00:03\t\n",
      "\n",
      "56,00:00:01\t57,00:00:02\t59,00:00:02\t\n",
      "\n",
      "\n",
      "58,00:00:03\t60,00:00:02\t\n",
      "\n",
      "61,00:00:03\t65,00:00:03\t63,00:00:03\t64,00:00:03\t\n",
      "\n",
      "\n",
      "\n",
      "62,00:00:03\t\n",
      "68,00:00:03\t66,00:00:03\t69,00:00:03\t\n",
      "\n",
      "\n",
      "70,00:00:03\t67,00:00:04\t\n",
      "\n",
      "71,00:00:02\t73,00:00:02\t72,00:00:03\t\n",
      "\n",
      "74,00:00:03\t\n",
      "\n",
      "75,00:00:02\t\n",
      "76,00:00:01\t77,00:00:01\t\n",
      "\n",
      "78,00:00:02\t\n",
      "79,00:00:04\t81,00:00:02\t80,00:00:02\t\n",
      "\n",
      "82,00:00:03\t\n",
      "\n",
      "83,00:00:01\t\n",
      "84,00:00:01\t\n",
      "85,00:00:02\t86,00:00:02\t\n",
      "\n",
      "87,00:00:02\t\n",
      "88,00:00:02\t\n",
      "89,00:00:02\t\n",
      "92,00:00:02\t91,00:00:02\t90,00:00:03\t\n",
      "\n",
      "\n",
      "93,00:00:03\t94,00:00:01\t\n",
      "\n",
      "95,00:00:03\t96,00:00:03\t\n",
      "97,00:00:03\t\n",
      "\n",
      "98,00:00:03\t99,00:00:03\t\n",
      "\n",
      "104,00:00:03\t\n",
      "101,00:00:03\t100,00:00:03\t103,00:00:03\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"Data\\data_big\\\\\"\n",
    "files = os.listdir(path)\n",
    "master_playlists = list()\n",
    "master_songs_set = set()\n",
    "master_songs_records = list()\n",
    "\n",
    "def process_file(file):\n",
    "    start_time = time.time()\n",
    "    file_row = files.index(file)\n",
    "    file_contents_json = open(path + file)\n",
    "    file_contents = json.load(file_contents_json)\n",
    "    playlists_in_file = file_contents['playlists']\n",
    "\n",
    "    for playlist in playlists_in_file:\n",
    "        playlist_name = dict(playlist)['name']\n",
    "        songs = list()\n",
    "        \n",
    "        for song in dict(playlist)['tracks']:\n",
    "            song_name = dict(song)['track_name']\n",
    "            songs.append(song_name)\n",
    "            master_songs_set.add(song_name)\n",
    "            master_songs_records.append((song_name, playlist_name))\n",
    "            \n",
    "        master_playlists.append(playlist_name)\n",
    "\n",
    "    del playlists_in_file\n",
    "    print(str(file_row) + \",\" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)) + \"\\t\")\n",
    "\n",
    "def process_all_files(file_list):\n",
    "    start_time = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(process_file, file_list)\n",
    "    print(\"all files processed in \" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)) + \"\\t(\" + str(len(master_playlists)) + \" playlists, \" + str(len(master_songs_set)) + \" songs)\")\n",
    "\n",
    "process_all_files(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting all of the data from the file, we create a sparse matrix with $m$ rows and $n$ columns where $m$ is the number of playlists and $n$ is the number of songs. This matrix is a sparse matrix because there are very few entries in the matrix which contain nonzero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_songs_set_list = list(master_songs_set)\n",
    "master_songs_vector = np.asarray(master_songs_set_list)\n",
    "m = len(master_playlists)\n",
    "n = len(master_songs_set_list)\n",
    "matrix = scipy.sparse.dok_matrix((m,n), dtype=int)\n",
    "\n",
    "start_time_total = time.time()\n",
    "counter = 0\n",
    "progress = pb.ProgressBar(widgets=[pb.Percentage(),\"\\t\", pb.Bar(),\"\\t\", pb.Timer(), \"\\tTotal Completed: \", pb.Counter()], maxval=len(master_songs_records)).start()\n",
    "for record in master_songs_records:\n",
    "    start_time = time.time()\n",
    "    matrix[master_playlists.index(record[1]),master_songs_set_list.index(record[0])] = 1\n",
    "    counter  = counter + 1\n",
    "    progress.update(counter)\n",
    "print(\"all playlists processed in \" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time_total)))\n",
    "\n",
    "del master_songs_set\n",
    "del master_songs_set_list\n",
    "del master_songs_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the sparse matrix, we convert it from a DOK (dictionary of keys) matrix to a CSC (compressed sparse column) matrix in order to create an npz file from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = scipy.sparse.csr_matrix(matrix)\n",
    "scipy.sparse.save_npz(\"data_train.npz\", sparse_matrix, \"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The npz file is then reloaded into the program for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = scipy.sparse.load_npz(\"data_train.npz\")\n",
    "print(sparse_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Importing Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same procedures and code to choose test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data_big\\\\\"\n",
    "files = os.listdir(path)\n",
    "master_playlists = list()\n",
    "master_songs_set = set()\n",
    "master_songs_records = list()\n",
    "\n",
    "def process_file(file):\n",
    "    start_time = time.time()\n",
    "    file_row = files.index(file)\n",
    "    file_contents_json = open(path + file)\n",
    "    file_contents = json.load(file_contents_json)\n",
    "    playlists_in_file = file_contents['playlists']\n",
    "\n",
    "    for playlist in playlists_in_file:\n",
    "        playlist_name = dict(playlist)['name']\n",
    "        songs = list()\n",
    "        \n",
    "        for song in dict(playlist)['tracks']:\n",
    "            song_name = dict(song)['track_name']\n",
    "            songs.append(song_name)\n",
    "            master_songs_set.add(song_name)\n",
    "            master_songs_records.append((song_name, playlist_name))\n",
    "            \n",
    "        master_playlists.append(playlist_name)\n",
    "\n",
    "    del playlists_in_file\n",
    "    print(str(file_row) + \",\" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)) + \"\\t\")\n",
    "\n",
    "def process_all_files(file_list):\n",
    "    start_time = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(process_file, file_list)\n",
    "    print(\"all files processed in \" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)) + \"\\t(\" + str(len(master_playlists)) + \" playlists, \" + str(len(master_songs_set)) + \" songs)\")\n",
    "\n",
    "process_all_files(files[100:103])\n",
    "\n",
    "del files\n",
    "\n",
    "master_songs_set_list = list(master_songs_set)\n",
    "master_songs_vector = np.asarray(master_songs_set_list)\n",
    "m = len(master_playlists)\n",
    "n = len(master_songs_set_list)\n",
    "matrix = scipy.sparse.dok_matrix((m,n), dtype=int)\n",
    "\n",
    "start_time_total = time.time()\n",
    "counter = 0\n",
    "progress = pb.ProgressBar(widgets=[pb.Percentage(),\"\\t\", pb.Bar(),\"\\t\", pb.Timer(), \"\\tTotal Completed: \", pb.Counter()], maxval=len(master_songs_records)).start()\n",
    "for record in master_songs_records:\n",
    "    start_time = time.time()\n",
    "    matrix[master_playlists.index(record[1]),master_songs_set_list.index(record[0])] = 1\n",
    "    counter  = counter + 1\n",
    "    progress.update(counter)\n",
    "print(\"all playlists processed in \" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time_total)))\n",
    "\n",
    "sparse_matrix_test = scipy.sparse.csr_matrix(matrix)\n",
    "scipy.sparse.save_npz(\"data_test.npz\", sparse_matrix_test, \"int32\")\n",
    "\n",
    "sparse_matrix_test = scipy.sparse.load_npz(\"data_test.npz\")\n",
    "print(sparse_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Penalizing for Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we explored the introduction of a tunable hyperparameter in order to penalize songs for greater popularity. We defined a function which takes the parameters $X$ (the data matrix), and $\\zeta$ (zeta, the hyperparameter) and scales the elements of the data matrix to penalize for popularity. We used the following formula to as a scale:\n",
    "$$z = \\frac{1-\\zeta}{\\zeta}$$\n",
    "We also restricted $\\zeta$ to $\\zeta \\neq 0$, so if $\\zeta = 0$, then the function does not scale the matrix and returns the original matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularityScale(X, zeta):\n",
    "    if zeta != 0:\n",
    "        freqs = np.sum(X, axis = 0)\n",
    "        z = (1-zeta)/zeta\n",
    "        return X * (np.reciprocal(np.power(np.amax(freqs), z)) * np.power(freqs, z))\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converted the sparse matrix back into a dense matrix in order to run the popularityScale function on the imported data. The data was then converted back to a sparse matrix for memory conservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of sparse_matrix:\", np.shape(sparse_matrix))\n",
    "dense_matrix = scipy.sparse.dok_matrix.toarray(sparse_matrix)\n",
    "scaled_matrix = scipy.sparse.dok_matrix(popularityScale(dense_matrix, 2))\n",
    "print(\"size of scaled_matrix:\", np.shape(scaled_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Running Alternating Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the scikit-learn library to run the Alternating Least Squares learning algorithm on the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors=200, use_gpu=False)\n",
    "model.fit(scaled_matrix.T)\n",
    "print(scaled_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
