{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Collaborative Filtering and the Million Playlist Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justin Moczynski, Gabe Pesco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are songs which are famous and appear on many playlists. There is also a significantly larger amount of songs which are unknown to the public eye. How do the lesser known tracks have a chance at becoming more popular? How can these songs be recommended to listeners in such a way that both popularity bias is reduced and the songs are suggested to an accurate set of listeners? In this project, we explored how to reduce the popularity bias in recommending music to listeners using a variation on a recommender system tasked with suggesting tracks to be added to a playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this project was the Million Playlist Dataset (MPD) provided by Spotify for their Recommender Systems Challenge. The first obstacle was importing the data and converting it from a JSON (JavaScript Object Notation) file to a npz file storing a compressed sparse row matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "This code should only run once for the entire project because its purpose is to extract data and convert it into an npz file for use during the program. Once the npz file has been created, it does not need to be created multiple times since we hold a reference to these files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the following libraries were imported to use in the data conversion process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import implicit\n",
    "import scipy.sparse\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "from sklearn import *\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Importing Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we began importing the data from the .json files. In order to do this, we used file scanners to read the contents of each file and dictionary, list, and set data structures to contain the scanned contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"Data\\\\data_big\\\\\"\n",
    "files = os.listdir(path)\n",
    "master_playlists = list()\n",
    "master_songs_set = set()\n",
    "master_songs_records = list()\n",
    "\n",
    "def process_file(file):\n",
    "    start_time = time.time()\n",
    "    file_row = files.index(file)\n",
    "    file_contents_json = open(path + file)\n",
    "    file_contents = json.load(file_contents_json)\n",
    "    playlists_in_file = file_contents['playlists']\n",
    "\n",
    "    for playlist in playlists_in_file:\n",
    "        playlist_name = dict(playlist)['name']\n",
    "        songs = list()\n",
    "        \n",
    "        for song in dict(playlist)['tracks']:\n",
    "            song_name = dict(song)['track_name']\n",
    "            songs.append(song_name)\n",
    "            master_songs_set.add(song_name)\n",
    "            master_songs_records.append((song_name, playlist_name))\n",
    "            \n",
    "        master_playlists.append(playlist_name)\n",
    "\n",
    "    del playlists_in_file\n",
    "    print(str(file_row) + \",\" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)) + \"\\t\")\n",
    "\n",
    "def process_all_files(file_list):\n",
    "    start_time = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=500) as executor:\n",
    "        executor.map(process_file, file_list)\n",
    "    print(\"all files processed in \" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)) + \"\\t(\" + str(len(master_playlists)) + \" playlists, \" + str(len(master_songs_set)) + \" songs)\")\n",
    "\n",
    "process_all_files(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting all of the data from the file, we create a sparse matrix with $m$ rows and $n$ columns where $m$ is the number of playlists and $n$ is the number of songs. This matrix is a sparse matrix because there are very few entries in the matrix which contain nonzero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_songs_set_list = list(master_songs_set)\n",
    "master_songs_vector = np.asarray(master_songs_set_list)\n",
    "m = len(master_playlists)\n",
    "n = len(master_songs_set_list)\n",
    "matrix = scipy.sparse.dok_matrix((m,n), dtype=int)\n",
    "\n",
    "start_time_total = time.time()\n",
    "counter = 0\n",
    "for record in master_songs_records:\n",
    "    start_time = time.time()\n",
    "    matrix[master_playlists.index(record[1]),master_songs_set_list.index(record[0])] = 1\n",
    "print(\"all playlists processed in \" + time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time_total)))\n",
    "\n",
    "del master_songs_set\n",
    "del master_songs_set_list\n",
    "del master_songs_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the sparse matrix, we convert it from a DOK (dictionary of keys) matrix to a CSC (compressed sparse column) matrix in order to create an npz file from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_save_name = \"data_train.npz\"\n",
    "sparse_matrix = scipy.sparse.csr_matrix(matrix)\n",
    "scipy.sparse.save_npz(file_save_name, sparse_matrix, \"int32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The npz file is then reloaded into the program for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = scipy.sparse.load_npz(file_save_name)\n",
    "print(sparse_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Penalizing for Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we explored the introduction of a tunable hyperparameter in order to penalize songs for greater popularity. We defined a function which takes the parameters $X$ (the data matrix), and $\\zeta$ (zeta, the hyperparameter) and scales the elements of the data matrix to penalize for popularity. We used the following formula to as a scale:\n",
    "$$z = \\frac{1-\\zeta}{\\zeta}$$\n",
    "We also restricted $\\zeta$ to $\\zeta \\neq 0$, so if $\\zeta = 0$, then the function does not scale the matrix and returns the original matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularityScale(X, zeta):\n",
    "    #frequencies of each track\n",
    "    freqs = np.sum(X, axis = 0)\n",
    "    #popularity scaling term is the nth root over the original\n",
    "    z = (1-zeta)/zeta\n",
    "    #scalar to make it so the most popular song is populated with 1s \n",
    "    scalar = np.reciprocal(np.amax(freqs)**z)\n",
    "    #applying frequency scaling to the matrix\n",
    "    return X * (scalar * (freqs**z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converted the sparse matrix back into a dense matrix in order to run the popularityScale function on the imported data. The data was then converted back to a sparse matrix for memory conservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of sparse_matrix:\", np.shape(sparse_matrix))\n",
    "dense_matrix = scipy.sparse.dok_matrix.toarray(sparse_matrix)\n",
    "scaled_matrix = scipy.sparse.dok_matrix(popularityScale(dense_matrix, 2))\n",
    "print(\"size of scaled_matrix:\", np.shape(scaled_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Running Alternating Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the scikit-learn library to run the Alternating Least Squares learning algorithm on the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTests():\n",
    "    users=100\n",
    "    n=5\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=224, use_gpu=False)\n",
    "    raw = scipy.sparse.load_npz(\"Data\\data10.npz\")\n",
    "    songlist = np.zeros(500)\n",
    "    rawsonglist = np.zeros(500)\n",
    "    scorelist = np.zeros(500)\n",
    "    rawscorelist = np.zeros(500)\n",
    "    for matrixes in range(10,21):\n",
    "        string = \"Data\\data\"+str(matrixes)\n",
    "        string+=\".npz\"\n",
    "        mat = scipy.sparse.load_npz(string)\n",
    "        model.fit(mat.T)\n",
    "\n",
    "        for row in range(users):\n",
    "            recs = model.recommend(row, mat, N=n, recalculate_user=True)\n",
    "            rawrecs = model.recommend(row, raw, N=n, recalculate_user=True)\n",
    "            for i in range(n):\n",
    "                rawsonglist[n*row+i]=rawrecs[i][0]\n",
    "                rawscorelist[n*row+i]=rawrecs[i][1]\n",
    "                songlist[n*row+i]=recs[i][0]\n",
    "                scorelist[n*row+i]=recs[i][1]\n",
    "        if(matrixes==10):\n",
    "            baseline = np.average(scorelist)\n",
    "            print(\"Baseline score:\",baseline)\n",
    "        print(\"Zeta parameter:\",matrixes/10)\n",
    "        print(\"number of unique songs recommended:\",np.size(np.unique(songlist)))\n",
    "        print(\"baseline - raw avg:\",baseline-np.average(rawscorelist))\n",
    "        print(\"error:\",100*((baseline-np.average(rawscorelist))/baseline),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:07<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.27471219559\n",
      "Zeta parameter: 1.0\n",
      "number of unique songs recommended: 357\n",
      "baseline - raw avg: 0.0\n",
      "error: 0.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:07<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.1\n",
      "number of unique songs recommended: 380\n",
      "baseline - raw avg: -0.00385210696515\n",
      "error: -1.40223369293 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:07<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.2\n",
      "number of unique songs recommended: 401\n",
      "baseline - raw avg: 0.000174127082234\n",
      "error: 0.0633852755825 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:08<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.3\n",
      "number of unique songs recommended: 405\n",
      "baseline - raw avg: 0.00499053823414\n",
      "error: 1.81664240403 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:08<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.4\n",
      "number of unique songs recommended: 409\n",
      "baseline - raw avg: 0.0101540747085\n",
      "error: 3.69625916558 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:08<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.5\n",
      "number of unique songs recommended: 414\n",
      "baseline - raw avg: 0.0154259174739\n",
      "error: 5.61530129407 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:08<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.6\n",
      "number of unique songs recommended: 419\n",
      "baseline - raw avg: 0.0208459067879\n",
      "error: 7.58827133361 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:08<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.7\n",
      "number of unique songs recommended: 426\n",
      "baseline - raw avg: 0.0257824535697\n",
      "error: 9.38525991332 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:07<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.8\n",
      "number of unique songs recommended: 432\n",
      "baseline - raw avg: 0.0302018116404\n",
      "error: 10.9939828392 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:07<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 1.9\n",
      "number of unique songs recommended: 432\n",
      "baseline - raw avg: 0.0346597994064\n",
      "error: 12.6167676436 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:08<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeta parameter: 2.0\n",
      "number of unique songs recommended: 435\n",
      "baseline - raw avg: 0.038843041289\n",
      "error: 14.1395401852 %\n"
     ]
    }
   ],
   "source": [
    "runTests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Potential Sources of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project required a complex implementation. There are many potential sources of error, including the data importing process and inaccuracy in the model due to the amount of data used for testing. The dip below zero error in our testing (indicating that the recommendations of the matrix with a value of 1.1 used for regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we would like to be able to import the entire Million Playlist Dataset into a sparse matrix once. In addition, we would like to continue experimenting with the induced hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
